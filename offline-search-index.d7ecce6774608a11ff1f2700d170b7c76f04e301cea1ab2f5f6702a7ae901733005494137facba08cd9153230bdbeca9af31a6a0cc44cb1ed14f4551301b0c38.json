[{"body":"About FSM FSM stands for Flomesh Service Mesh, an acronym for the first letters of the name. It is used for north-south traffic management for Kubernetes. With programmable proxy Pipy as its core, FSM provides features such as Ingress controller, Gateway API implementation, load balancer, and cross-cluster service discovery and registration.\n The goal of FSM is to manage traffic both within and between Kubernetes clusters.\n ","excerpt":"About FSM FSM stands for Flomesh Service Mesh, an acronym for the first letters of the name. It is used for north-south traffic management for Kubernetes. With programmable proxy Pipy as its core, FSM …","ref":"/docs/overview/about/","title":"About FSM"},{"body":"Service is a resource type in Kubernetes used to expose a group of Pod applications as a network service. Each Pod has its own IP, but this IP has the same lifespan as the Pod, which means that when the Pod is destroyed, the IP becomes invalid (it may also be allocated to other Pods). On the other hand, the IP ClusterIP of the Service does not change after creation. The Service is associated with the Pod through means such as userspace proxy, iptables, and ipvs proxy.\nLoadBalancer is one of four types of Service, the others being ClusterIP, NodePort, and ExternalName.\nThe operation of LoadBalancer requires collaboration with a third-party load balancer. When we install the Ingress controller, a Service of type LoadBalancer is created. The EXTERNAL-IP status of the newly created Service is pending, and if there is no load balancer, it will remain in the pending state.\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE fsm-ingress-pipy-controller LoadBalancer 10.43.188.208 \u003cpending\u003e 80:30508/TCP 8s FSM comes with a built-in Service Load Balancer (referred to as ServiceLB) that provides load balancing capabilities.\nInternals of ServiceLB How ServiceLB works can be understood by 2 bullet points below:\n At runtime, it listens for changes in the Service in the cluster, and if the Service type is LoadBalancer, it creates a Daemonset and inputs information such as the ClusterIP, Protocol, Port, etc. of the Service. The rest of the work is done by Daemonset resource initated by ServiceLB. When processing traffic, it uses the host port (which is the same as the Service port number in pod.spec.containers.ports.hostPort) to receive traffic and uses iptables to forward the received traffic to the ClusterIP of the Service.  Installation and Usage Using FSM ServiceLB is very simple, just provide the parameter --set fsm.serviceLB.enabled=true during installation to enable ServiceLB.\nhelm repo add fsm https://flomesh-io.github.io/fsm helm repo update helm install \\  --namespace fsm \\  --create-namespace \\  --set fsm.version=0.2.1-alpha.1 \\  --set fsm.serviceLB.enabled=true \\  fsm fsm/fsm After FSM is installed and running, you can find the Daemonset it created.\nkubectl get ds -n fsm NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE svclb-fsm-ingress-pipy-controller-37e63610 1 1 1 1 1 \u003cnone\u003e 3m At the same time, you can see the assigned EXTERNAL-IP on the ingress controller service.\nkubectl get svc -n fsm -l app.kubernetes.io/component=controller NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE fsm-ingress-pipy-controller LoadBalancer 10.43.144.20 10.0.0.12 80:30508/TCP 4h10m You can access the application using the EXTERNAL-IP.\n","excerpt":"Service is a resource type in Kubernetes used to expose a group of Pod applications as a network service. Each Pod has its own IP, but this IP has the same lifespan as the Pod, which means that when …","ref":"/docs/guides/service_loadbalancer/","title":"Flomesh Service Loadbalancer"},{"body":"","excerpt":"","ref":"/docs/overview/","title":"Overview of FSM"},{"body":"Setup FSM This guide explains the procedure required to install FSM component(s) on a running Kubernetes cluster.\nPrerequisites  a cluster running Kubernetes v1.19.0 or greater (using a cloud provider of choice, minikube, or similar) a workstation capable of executing Bash scripts The Kubernetes command-line tool - kubectl the helm cli available locally  Install via Helm export FSM_VERSION=0.2.0 export FSM_NAMESPACE=fsm helm install fsm fsm/fsm --namespace $FSM_NAMESPACE --create-namespace --version $FSM_VERSION Make sure that all pods are up and running.\nkubectl get po -n $FSM_NAMESPACE NAME READY STATUS RESTARTS AGE fsm-repo-59fc7c8fb6-fvscv 1/1 Running 0 2m5s fsm-manager-5bcfd7bd9d-pxfz9 1/1 Running 0 2m5s fsm-ingress-pipy-64f9df95f6-56t4m 1/1 Running 0 2m5s","excerpt":"Setup FSM This guide explains the procedure required to install FSM component(s) on a running Kubernetes cluster.\nPrerequisites  a cluster running Kubernetes v1.19.0 or greater (using a cloud provider …","ref":"/docs/getting_started/setup_fsm/","title":"Setup FSM"},{"body":"All Documentation Releases Active Releases  v0.2  ","excerpt":"All Documentation Releases Active Releases  v0.2  ","ref":"/docs/releases/docs/","title":"All Documentation Releases"},{"body":"In a Kubernetes cluster, there are two common modes for applications to access external services: one mode is for each application to complete the network communication with external services independently; the other mode is through a unified egress gateway for relay.\nFSM is widely used to handle north-south traffic in Kubernetes, and besides providing management of ingress traffic, egress gateway is another feature of it.\nWhen installing FSM, to enable the Egress Gateway functionality, you need to set the parameter --set fsm.egressGateway.enabled=true. More configuration parameters related to the Egress Gateway can be found here: https://github.com/flomesh-io/fsm/blob/main/charts/fsm/values.yaml#L268.\nhelm repo add fsm https://charts.flomesh.io helm repo update helm install --namespace fsm --create-namespace \\  --set fsm.version=0.2.1-alpha.2 \\  --set fsm.egressGateway.enabled=true \\  fsm fsm/fsm The connection between the application and the gateway has two modes, which can be set through the parameter --set fsm.egressGateway.mode=xxx during installation:\n http2tunnel uses HTTP/2 tunnel for communication and is the default mode sock5 uses the SOCKS5 protocol for communication.   HTTP Tunneling is a technique used to establish a network connection between two computers in cases where the connection is restricted by firewalls, NATs, ACLs, and other limitations. This tunnel is typically intermediated by a proxy server located in a DMZ.\n  SOCKS is a network protocol that acts as an intermediary between a client and a server to facilitate communication between the two. It is primarily used to bypass network restrictions such as firewalls, NATs, and ACLs and typically operates at the session layer of the OSI model, between the presentation layer and the transport layer.\n Flomesh Egress Gateway is often integrated with service mesh osm-edge to provide more intelligent and efficient outbound traffic management. For more information and integration guidance, please refer to osm-edge Egress Gateway usage guide\n","excerpt":"In a Kubernetes cluster, there are two common modes for applications to access external services: one mode is for each application to complete the network communication with external services …","ref":"/docs/guides/egress_gateway/","title":"Flomesh Egress Gateway"},{"body":"FSM Components This guide explains the components of Flomesh Service Mesh (FSM)\nInspect Components of FSM Some FSM components will be installed in the designated namespace during the installation process, the default namespace is fsm. It can be checked by the following command:\nkubectl get pods,svc,secrets,serviceaccount -n fsm Components With the default configuration, the following components will be installed:\n(1) Manager The FSM Manager is the core component of FSM and almost all other components interact directly or indirectly with the manager. The manager continuously monitors updates to Kubernetes native resources (Service, Node, Ingress, etc.) and FSM custom resources (Cluster, GlobalTrafficPolicy, ServiceExport, ServiceImport, etc. related to multi-cluster) and synchronizes the configuration with other components.\n(2) Repository The FSM repository (a.k.a repo) is a concept in Pipy, which stores the scripts (logic code) and configurations required for the data plane components to process traffic. These scripts and configurations come from the manager and the repository provides a REST API for interaction. The manager updates the scripts and configurations through the REST API; the data plane proxies are notified for changes in repository codebase.\n(3) Ingress Controller The FSM Ingress Controller (a.k.a Flomesh Ingress) is one of the data planes of FSM, with the programmable proxy Pipy as its core, it implements the Kubernetes Ingress Controller API to provide management of Kubernetes ingress traffic. As one of the data plane components, it continuously monitors the repository (repo) and retrieves routing and other configuration information from the repo. For more information on using the Flomesh ingress controller, see the documentation.\nIn addition to the components installed by default, there are few other optional components.\n(1) Egress Gateway The Egress Gateway, as its name suggests, is a gateway for outgoing traffic, and all outgoing traffic is routed through a unified Egress Gateway. Outgoing policies can be implemented on the Egress Gateway to manage outgoing traffic. For more information on using the Flomesh Egress Gateway, see the Learn how to use FSM Egress Gateway component guide.\n(2) FSM Service Load balancer FSM Service Load Balancer is a component in FSM that provides external IP for LoadBalancer type Services. All traffic that enters through the external IP is load balanced to the Endpoints associated with the Service for processing. For more information refer to guide Learn how to use FSM Service load balancing component\n","excerpt":"FSM Components This guide explains the components of Flomesh Service Mesh (FSM)\nInspect Components of FSM Some FSM components will be installed in the designated namespace during the installation …","ref":"/docs/overview/components/","title":"FSM Components"},{"body":"","excerpt":"","ref":"/docs/quickstart/","title":"Quick Start"},{"body":"FSM offers an implementation of Kubernetes Multi-Cluster Service API (MCS) that depends on a service mesh and Ingress to provide service registration and discovery capabilities across k8s clusters. When using FSM as an MCS provider, it must be used in conjunction with osm-edge, which provides east-west traffic flow capabilities, while FSM provides north-south traffic flow capabilities. FSM also provides the ability to register and discover services across clusters. In the MCS multi-cluster mode, FSM currently supports three traffic modes for Kubernetes clusters that join the ClusterSet: Locality, Failover, and ActiveActive.\nIn the Locality mode, which is the default configuration, traffic is scheduled based on proximity within the local cluster or can be scheduled to a specific cluster based on topology.In the ActiveActive mode, the cluster provides the same traffic processing capabilities as service export (ServiceExport) using load balancing. In the Failover mode, the importing service (ServiceImport) cluster provides disaster recovery capabilities for the exporting service (ServiceExport) cluster. FSM’s implementation does not require the underlying Kubernetes to use a special network and does not require a unified two-layer or three-layer network between the multi-cluster networks. Therefore, it is generally applicable to Kubernetes clusters of various types of networks.\nRefer to osm-edge integration with FSM MCS guide \u0026 demo\nCluster CRD When registering a cluster, we provide the following information.\n The address (e.g. gatewayHost: cluster-A.host) and port (e.g. gatewayPort: 80) of the cluster kubeconfig to access the cluster, containing the api-server address and information such as the certificate and secret key  apiVersion:flomesh.io/v1alpha1kind:Clustermetadata:name:cluster-Aspec:gatewayHost:cluster-A.hostgatewayPort:80kubeconfig:|+--- apiVersion: v1 clusters: - cluster: certificate-authority-data: server: https://cluster-A.host:6443 name: cluster-A contexts: - context: cluster: cluster-A user: admin@cluster-A name: cluster-A current-context: cluster-A kind: Config preferences: {} users: - name: admin@cluster-A user: client-certificate-data: client-key-data:ServiceExport and ServiceImport CRD For cross-cluster service registration, FSM provides the ServiceExport and ServiceImport CRDs from KEP-1645: Multi-Cluster Services API for ServiceExports.flomesh.io and ServiceImports.flomesh.io. The former is used to register services with the control plane and declare that the application can provide services across clusters, while the latter is used to reference services from other clusters.\nFor clusters cluster-A and cluster-B that join the cluster federation, a Service named foo exists under the namespace bar of cluster cluster-A and a ServiceExport foo of the same name is created under the same namespace. A ServiceImport resource with the same name is automatically created under the namespace bar of cluster cluster-B (if it does not exist, it is automatically created).\n// in cluster-AapiVersion:v1kind:Servicemetadata:name:foonamespace:barspec:ports:- port:80selector:app:foo---apiVersion:flomesh.io/v1alpha1kind:ServiceExportmetadata:name:foonamespace:bar---// in cluster-BapiVersion:flomesh.io/v1alpha1kind:ServiceImportmetadata:name:foonamespace:barThe YAML snippet above shows how to register the foo service to the control plane of a multi-cluster. In the following, we will walk through a slightly more complex scenario of cross-cluster service registration and traffic scheduling.\nFor detailed CRD reference, refer to Multicluster API Reference\nDemo  Multi-cluster services discovery \u0026 communication Multi-cluster services access control  ","excerpt":"FSM offers an implementation of Kubernetes Multi-Cluster Service API (MCS) that depends on a service mesh and Ingress to provide service registration and discovery capabilities across k8s clusters. …","ref":"/docs/guides/multicluster/","title":"Flomesh Multi-cluster API"},{"body":"","excerpt":"","ref":"/docs/getting_started/","title":"Getting Started"},{"body":"","excerpt":"","ref":"/docs/guides/","title":"How-to Guides"},{"body":"","excerpt":"","ref":"/docs/demos/","title":"Demos"},{"body":"","excerpt":"","ref":"/docs/api_rerence/","title":"API Reference"},{"body":"","excerpt":"","ref":"/docs/releases/","title":"Releases"},{"body":"Demo Architecture For demonstration purposes we will be creating 4 Kubernetes clusters and high-level architecture will look something like the below:\n As a convention and for this demo we will be creating a separate stand-alone cluster to serve as a control plane cluster, but that isn’t strictly required as a separate cluster and it could be one of any existing cluster.\n Pre-requisites  kubectx: for switching between multiple kubeconfig contexts (clusters) k3d: for creating multiple k3s clusters locally using containers helm: for deploying FSM docker: required to run k3d Have osm CLI available for managing the service mesh. OSM version \u003e= v1.2.0.  Demo clusters \u0026 environment setup In this demo, we will be using k3d a lightweight wrapper to run k3s (Rancher Lab’s minimal Kubernetes distribution) in docker, to create 4 separate clusters named control-plane, cluster-1, cluster-2, and cluster-3 respectively.\nWe will be using the HOST machine IP address and separate ports during the installation, for us to easily access the individual clusters. My demo host machine IP address is 192.168.1.110 (it might be different for your machine).\n   cluster cluster ip api-server port LB external-port description     control-plane HOST_IP(192.168.1.110) 6444 N/A control-plane cluster   cluster-1 HOST_IP(192.168.1.110) 6445 81 application-cluster   cluster-2 HOST_IP(192.168.1.110) 6446 82 Application Cluster   cluster-3 HOST_IP(192.168.1.110) 6447 83 Application Cluster    Network Creates a docker bridge type network named multi-clusters, which run all containers.\ndocker network create multi-clusters Find your machine host IP address, mine is 192.168.1.110, and export that as an environment variable to be used later.\nexport HOST_IP=192.168.1.110 Cluster creation We are going to use k3d to create 4 clusters.\nAPI_PORT=6444 #6444 6445 6446 6447 PORT=80 #81 82 83 for CLUSTER_NAME in control-plane cluster-1 cluster-2 cluster-3 do k3d cluster create ${CLUSTER_NAME} \\  --image docker.io/rancher/k3s:v1.23.8-k3s2 \\  --api-port \"${HOST_IP}:${API_PORT}\" \\  --port \"${API_PORT}:6443@server:0\" \\  --port \"${PORT}:80@server:0\" \\  --servers-memory 4g \\  --k3s-arg \"--disable=traefik@server:0\" \\  --network multi-clusters \\  --timeout 120s \\  --wait ((API_PORT=API_PORT+1)) ((PORT=PORT+1)) done Install FSM Install FSM to newly created 4 clusters.\nhelm repo update export FSM_NAMESPACE=flomesh export FSM_VERSION=0.2.0-alpha.9 for CLUSTER_NAME in control-plane cluster-1 cluster-2 cluster-3 do kubectx k3d-${CLUSTER_NAME} sleep 1 helm install --namespace ${FSM_NAMESPACE} --create-namespace --version=${FSM_VERSION} --set fsm.logLevel=5 fsm fsm/fsm sleep 1 kubectl wait --for=condition=ready pod --all -n $FSM_NAMESPACE done We have our clusters ready, now we need to federate them together, but before we do that, let’s first understand the mechanics on how FSM is configured.\nFederate clusters We will enroll clusters cluster-1, cluster-2, and cluster-3 into the management of control-plane cluster.\nexport HOST_IP=192.168.1.110 kubectx k3d-control-plane sleep 1 PORT=81 for CLUSTER_NAME in cluster-1 cluster-2 cluster-3 do cat \u003c\u003cEOF apiVersion: flomesh.io/v1alpha1 kind: Cluster metadata: name: ${CLUSTER_NAME} spec: gatewayHost: ${HOST_IP} gatewayPort: ${PORT} kubeconfig: |+ `k3d kubeconfig get ${CLUSTER_NAME} | sed 's|^| |g' | sed \"s|0.0.0.0|$HOST_IP|g\"` EOF ((PORT=PORT+1)) done Install osm-edge Service Mesh Download osm CLI\nInstall the service mesh osm-edge to the clusters cluster-1, cluster-2, and cluster-3. The control plane does not handle application traffic and does not need to be installed.\nexport OSM_NAMESPACE=osm-system export OSM_MESH_NAME=osm for CLUSTER_NAME in cluster-1 cluster-2 cluster-3 do kubectx k3d-${CLUSTER_NAME} DNS_SVC_IP=\"$(kubectl get svc -n kube-system -l k8s-app=kube-dns -o jsonpath='{.items[0].spec.clusterIP}')\" osm install \\  --mesh-name \"$OSM_MESH_NAME\" \\  --osm-namespace \"$OSM_NAMESPACE\" \\  --set=osm.certificateProvider.kind=tresor \\  --set=osm.image.pullPolicy=Always \\  --set=osm.sidecarLogLevel=error \\  --set=osm.controllerLogLevel=warn \\  --timeout=900s \\  --set=osm.localDNSProxy.enable=true \\  --set=osm.localDNSProxy.primaryUpstreamDNSServerIPAddr=\"${DNS_SVC_IP}\" done Deploy Demo application Deploying mesh-managed applications Deploy the httpbin application under the httpbin namespace of clusters cluster-1 and cluster-3 (which are managed by the mesh and will inject sidecar). Here the httpbin application is implemented by Pipy and will return the current cluster name.\nexport NAMESPACE=httpbin for CLUSTER_NAME in cluster-1 cluster-3 do kubectx k3d-${CLUSTER_NAME} kubectl create namespace ${NAMESPACE} osm namespace add ${NAMESPACE} kubectl apply -n ${NAMESPACE} -f - \u003c\u003cEOF apiVersion: apps/v1 kind: Deployment metadata: name: httpbin labels: app: pipy spec: replicas: 1 selector: matchLabels: app: pipy template: metadata: labels: app: pipy spec: containers: - name: pipy image: flomesh/pipy:latest ports: - containerPort: 8080 command: - pipy - -e - | pipy() .listen(8080) .serveHTTP(new Message('Hi, I am from ${CLUSTER_NAME} and controlled by mesh!\\n')) --- apiVersion: v1 kind: Service metadata: name: httpbin spec: ports: - port: 8080 targetPort: 8080 protocol: TCP selector: app: pipy --- apiVersion: v1 kind: Service metadata: name: httpbin-${CLUSTER_NAME} spec: ports: - port: 8080 targetPort: 8080 protocol: TCP selector: app: pipy EOF sleep 3 kubectl wait --for=condition=ready pod -n ${NAMESPACE} --all --timeout=60s done Deploy the curl application under the namespace curl in cluster cluster-2, which is managed by the mesh.\nexport NAMESPACE=curl kubectx k3d-cluster-2 kubectl create namespace ${NAMESPACE} osm namespace add ${NAMESPACE} kubectl apply -n ${NAMESPACE} -f - \u003c\u003cEOF apiVersion: v1 kind: ServiceAccount metadata: name: curl --- apiVersion: v1 kind: Service metadata: name: curl labels: app: curl service: curl spec: ports: - name: http port: 80 selector: app: curl --- apiVersion: apps/v1 kind: Deployment metadata: name: curl spec: replicas: 1 selector: matchLabels: app: curl template: metadata: labels: app: curl spec: serviceAccountName: curl containers: - image: curlimages/curl imagePullPolicy: IfNotPresent name: curl command: [\"sleep\", \"365d\"] EOF sleep 3 kubectl wait --for=condition=ready pod -n ${NAMESPACE} --all --timeout=60s Export Service Let’s export services in cluster-1 and cluster-3\nexport NAMESPACE_MESH=httpbin for CLUSTER_NAME in cluster-1 cluster-3 do kubectx k3d-${CLUSTER_NAME} kubectl apply -f - \u003c\u003cEOF apiVersion: flomesh.io/v1alpha1 kind: ServiceExport metadata: namespace: ${NAMESPACE_MESH} name: httpbin spec: serviceAccountName: \"*\" rules: - portNumber: 8080 path: \"/${CLUSTER_NAME}/httpbin-mesh\" pathType: Prefix --- apiVersion: flomesh.io/v1alpha1 kind: ServiceExport metadata: namespace: ${NAMESPACE_MESH} name: httpbin-${CLUSTER_NAME} spec: serviceAccountName: \"*\" rules: - portNumber: 8080 path: \"/${CLUSTER_NAME}/httpbin-mesh-${CLUSTER_NAME}\" pathType: Prefix EOF sleep 1 done After exporting the services, FSM will automatically create Ingress rules for them, and with the rules, you can access these services through Ingress.\nfor CLUSTER_NAME_INDEX in 1 3 do CLUSTER_NAME=cluster-${CLUSTER_NAME_INDEX} ((PORT=80+CLUSTER_NAME_INDEX)) kubectx k3d-${CLUSTER_NAME} echo \"Getting service exported in cluster ${CLUSTER_NAME}\" echo '-----------------------------------' kubectl get serviceexports.flomesh.io -A echo '-----------------------------------' curl -s \"http://${HOST_IP}:${PORT}/${CLUSTER_NAME}/httpbin-mesh\" curl -s \"http://${HOST_IP}:${PORT}/${CLUSTER_NAME}/httpbin-mesh-${CLUSTER_NAME}\" echo '-----------------------------------' done To view one of the ServiceExports resources.\nkubectl get serviceexports httpbin -n httpbin -o jsonpath='{.spec}' | jq { \"loadBalancer\": \"RoundRobinLoadBalancer\", \"rules\": [ { \"path\": \"/cluster-3/httpbin-mesh\", \"pathType\": \"Prefix\", \"portNumber\": 8080 } ], \"serviceAccountName\": \"*\" } The exported services can be imported into other managed clusters. For example, if we look at the cluster cluster-2, we can have multiple services imported.\nkubectx k3d-cluster-2 kubectl get serviceimports -A NAMESPACE NAME AGE httpbin httpbin-cluster-1 13m httpbin httpbin-cluster-3 13m httpbin httpbin 13m Testing Staying in the cluster-2 cluster (kubectx k3d-cluster-2), we test if we can access these imported services from the curl application in the mesh.\nGet the pod of the curl application, from which we will later launch requests to simulate service access.\ncurl_client=\"$(kubectl get pod -n curl -l app=curl -o jsonpath='{.items[0].metadata.name}')\" At this point you will find that it is not accessible.\nkubectl exec \"${curl_client}\" -n curl -c curl -- curl -s http://httpbin.httpbin:8080/ command terminated with exit code 7 Note that this is normal, by default no other cluster instance will be used to respond to requests, which means no calls to other clusters will be made by default. So how to access it, then we need to be clear about the global traffic policy GlobalTrafficPolicy.\nGlobal Traffic Policy Note that all global traffic policies are set on the user’s side, so this demo is about setting global traffic policies on the cluster cluster-2 side. So before you start, switch to cluster cluster-2: kubectx k3d-cluster-2.\nThe global traffic policy is set via CRD GlobalTrafficPolicy.\ntype GlobalTrafficPolicy struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec GlobalTrafficPolicySpec `json:\"spec,omitempty\"` Status GlobalTrafficPolicyStatus `json:\"status,omitempty\"` } type GlobalTrafficPolicySpec struct { LbType LoadBalancerType `json:\"lbType\"` LoadBalanceTarget []TrafficTarget `json:\"targets\"` } Global load balancing types .spec.lbType There are three types.\n Locality: uses only the services of this cluster, and is also the default type. This is why accessing the httpbin application fails when we don’t provide any global policy, because there is no such service in cluster cluster-2. FailOver: proxies to other clusters only when access to this cluster fails, which is often referred to as failover, similar to primary backup. ActiveActive: Proxy to other clusters under normal conditions, similar to multi-live.  The FailOver and ActiveActive policies are used with the targets field to specify the id of the standby cluster, which is the cluster that can be routed to in case of failure or load balancing. ** For example, if you look at the import service httpbin/httpbin in cluster cluster-2, you can see that it has two endpoints for the outer cluster, note that endpoints here is a different concept than the native endpoints.v1 and will contain more information. In addition, there is the cluster id clusterKey.\nkubectl get serviceimports httpbin -n httpbin -o jsonpath='{.spec}' | jq { \"ports\": [ { \"endpoints\": [ { \"clusterKey\": \"default/default/default/cluster-1\", \"target\": { \"host\": \"192.168.1.110\", \"ip\": \"192.168.1.110\", \"path\": \"/cluster-1/httpbin-mesh\", \"port\": 81 } }, { \"clusterKey\": \"default/default/default/cluster-3\", \"target\": { \"host\": \"192.168.1.110\", \"ip\": \"192.168.1.110\", \"path\": \"/cluster-3/httpbin-mesh\", \"port\": 83 } } ], \"port\": 8080, \"protocol\": \"TCP\" } ], \"serviceAccountName\": \"*\", \"type\": \"ClusterSetIP\" } Routing Type - Locality The default routing type is Locality, and as tested above, traffic cannot be dispatched to other clusters.\nkubectl exec \"${curl_client}\" -n curl -c curl -- curl -s http://httpbin.httpbin:8080/ command terminated with exit code 7 Routing Type - FailOver Since setting a global traffic policy for causes access failure, we start by enabling FailOver mode. Note that the global policy traffic, to be consistent with the target service name and namespace. For example, if we want to access http://httpbin.httpbin:8080/, we need to create GlobalTrafficPolicy resource named httpbin under the namespace httpbin.\nkubectl apply -n httpbin -f - \u003c\u003cEOF apiVersion: flomesh.io/v1alpha1 kind: GlobalTrafficPolicy metadata: name: httpbin spec: lbType: FailOver targets: - clusterKey: default/default/default/cluster-1 - clusterKey: default/default/default/cluster-3 EOF After setting the policy, let’s try it again by requesting.\nkubectl exec \"${curl_client}\" -n curl -c curl -- curl -s http://httpbin.httpbin:8080/ Hi, I am from cluster-1! The request is successful and the request is proxied to the service in cluster cluster-1. Another request is made, and it is proxied to cluster cluster-3, as expected for load balancing.\nkubectl exec \"${curl_client}\" -n curl -c curl -- curl -s http://httpbin.httpbin:8080/ Hi, I am from cluster-3! What will happen if we deploy the application httpbin in the namespace httpbin of the cluster cluster-2?\nexport NAMESPACE=httpbin export CLUSTER_NAME=cluster-2 kubectx k3d-${CLUSTER_NAME} kubectl create namespace ${NAMESPACE} osm namespace add ${NAMESPACE} kubectl apply -n ${NAMESPACE} -f - \u003c\u003cEOF apiVersion: apps/v1 kind: Deployment metadata: name: httpbin labels: app: pipy spec: replicas: 1 selector: matchLabels: app: pipy template: metadata: labels: app: pipy spec: containers: - name: pipy image: flomesh/pipy:latest ports: - containerPort: 8080 command: - pipy - -e - | pipy() .listen(8080) .serveHTTP(new Message('Hi, I am from ${CLUSTER_NAME}!\\n')) --- apiVersion: v1 kind: Service metadata: name: httpbin spec: ports: - port: 8080 targetPort: 8080 protocol: TCP selector: app: pipy --- apiVersion: v1 kind: Service metadata: name: httpbin-${CLUSTER_NAME} spec: ports: - port: 8080 targetPort: 8080 protocol: TCP selector: app: pipy EOF sleep 3 kubectl wait --for=condition=ready pod -n ${NAMESPACE} --all --timeout=60s After the application is running normally, this time we send the request to test again. From the results, it looks like the request is processed in the current cluster.\nkubectl exec \"${curl_client}\" -n curl -c curl -- curl -s http://httpbin.httpbin:8080/ Hi, I am from cluster-2! Even if the request is repeated multiple times, it will always return Hi, I am from cluster-2!, which indicates that the services of same cluster are used in preference to the services imported from other clusters.\nIn some cases, we also want other clusters to participate in the service as well, because the resources of other clusters are wasted if only the services of this cluster are used. This is where the ActiveActive routing type comes into play.\nRouting Type - ActiveActive Moving on from the status above, let’s test the ActiveActive type by modifying the policy created earlier and updating it to ActiveActive: `ActiveActive\nkubectl apply -n httpbin -f - \u003c\u003cEOF apiVersion: flomesh.io/v1alpha1 kind: GlobalTrafficPolicy metadata: name: httpbin spec: lbType: ActiveActive targets: - clusterKey: default/default/default/cluster-1 - clusterKey: default/default/default/cluster-3 EOF Multiple requests will show that httpbin from all three clusters will participate in the service. This indicates that the load is being proxied to multiple clusters in a balanced manner.\nkubectl exec \"${curl_client}\" -n curl -c curl -- curl -s http://httpbin.httpbin:8080/ Hi, I am from cluster-1 and controlled by mesh! kubectl exec \"${curl_client}\" -n curl -c curl -- curl -s http://httpbin.httpbin:8080/ Hi, I am from cluster-2! kubectl exec \"${curl_client}\" -n curl -c curl -- curl -s http://httpbin.httpbin:8080/ Hi, I am from cluster-3 and controlled by mesh! ","excerpt":"Demo Architecture For demonstration purposes we will be creating 4 Kubernetes clusters and high-level architecture will look something like the below:\n As a convention and for this demo we will be …","ref":"/docs/demos/multicluster_services_communication/","title":"Multi-cluster services discovery \u0026 communication"},{"body":"Pre-requisites  Tools and clusters created in demo Multi-cluster services discovery \u0026 communication  This guide will expand on the knowledge we covered in previous guides and demonstrate how to configure and enable cross-cluster access control based on SMI. With osm-edge support for multi-clusters, users can define and enforce fine-grained access policies for services running across multiple Kubernetes clusters. This allows users to easily and securely manage access to services and resources, ensuring that only authorized users and applications have access to the appropriate services and resources.\nBefore we start, let’s review the SMI Access Control Specification. There are two forms of traffic policies: Permissive Mode and Traffic Policy Mode. The former allows services in the mesh to access each other, while the latter requires the provision of the appropriate traffic policy to be accessible.\nSMI Access Control Policy In traffic policy mode, SMI defines ServiceAccount-based access control through the Kubernetes Custom Resource Definition(CRD) TrafficTarget, which defines traffic sources (sources), destinations (destinations), and rules (rules). What is expressed is that applications that use the ServiceAccount specified in sources can access applications that have the ServiceAccount specified in destinations, and the accessible traffic is specified by rules.\nFor example, the following example represents a load running with ServiceAccount promethues sending a GET request to the /metrics endpoint of a load running with ServiceAccount service-a. The HTTPRouteGroup defines the identity of the traffic: i.e. the GET request to access the endpoint /metrics.\nkind:HTTPRouteGroupmetadata:name:the-routesspec:matches:- name:metricspathRegex:\"/metrics\"methods:- GET---kind:TrafficTargetmetadata:name:path-specificnamespace:defaultspec:destination:kind:ServiceAccountname:service-anamespace:defaultrules:- kind:HTTPRouteGroupname:the-routesmatches:- metricssources:- kind:ServiceAccountname:prometheusnamespace:defaultSo how does access control perform in a multi-cluster?\nFSM’s ServiceExport FSM’s ServiceExport is used to export services to other clusters, which is the process of service registration. The field spec.serviceAccountName of ServiceExport can be used to specify the ServiceAccount used for the service load.\napiVersion:flomesh.io/v1alpha1kind:ServiceExportmetadata:namespace:httpbinname:httpbinspec:serviceAccountName:\"*\"rules:- portNumber:8080path:\"/cluster-1/httpbin-mesh\"pathType:PrefixDeploy the application Deploy the sample application Deploy the httpbin application under the httpbin namespace (managed by the mesh, which injects sidecar) in clusters cluster-1 and cluster-3. Here we specify ServiceAccount as httpbin.\nexport NAMESPACE=httpbin for CLUSTER_NAME in cluster-1 cluster-3 do kubectx k3d-${CLUSTER_NAME} kubectl create namespace ${NAMESPACE} osm namespace add ${NAMESPACE} kubectl apply -n ${NAMESPACE} -f - \u003c\u003cEOF apiVersion: v1 kind: ServiceAccount metadata: name: httpbin --- apiVersion: apps/v1 kind: Deployment metadata: name: httpbin labels: app: pipy spec: replicas: 1 selector: matchLabels: app: pipy template: metadata: labels: app: pipy spec: serviceAccountName: httpbin containers: - name: pipy image: flomesh/pipy:latest ports: - containerPort: 8080 command: - pipy - -e - | pipy() .listen(8080) .serveHTTP(new Message('Hi, I am from ${CLUSTER_NAME} and controlled by mesh!\\n')) --- apiVersion: v1 kind: Service metadata: name: httpbin spec: ports: - port: 8080 targetPort: 8080 protocol: TCP selector: app: pipy --- apiVersion: v1 kind: Service metadata: name: httpbin-${CLUSTER_NAME} spec: ports: - port: 8080 targetPort: 8080 protocol: TCP selector: app: pipy EOF sleep 3 kubectl wait --for=condition=ready pod -n ${NAMESPACE} --all --timeout=60s done Deploy the httpbin application under the cluster-2 namespace httpbin, but do not specify a ServiceAccount and use the default ServiceAccount default.\nexport NAMESPACE=httpbin export CLUSTER_NAME=cluster-2 kubectx k3d-${CLUSTER_NAME} kubectl create namespace ${NAMESPACE} osm namespace add ${NAMESPACE} kubectl apply -n ${NAMESPACE} -f - \u003c\u003cEOF apiVersion: apps/v1 kind: Deployment metadata: name: httpbin labels: app: pipy spec: replicas: 1 selector: matchLabels: app: pipy template: metadata: labels: app: pipy spec: containers: - name: pipy image: flomesh/pipy:latest ports: - containerPort: 8080 command: - pipy - -e - | pipy() .listen(8080) .serveHTTP(new Message('Hi, I am from ${CLUSTER_NAME}! and controlled by mesh!\\n')) --- apiVersion: v1 kind: Service metadata: name: httpbin spec: ports: - port: 8080 targetPort: 8080 protocol: TCP selector: app: pipy --- apiVersion: v1 kind: Service metadata: name: httpbin-${CLUSTER_NAME} spec: ports: - port: 8080 targetPort: 8080 protocol: TCP selector: app: pipy EOF sleep 3 kubectl wait --for=condition=ready pod -n ${NAMESPACE} --all --timeout=60s Deploy the curl application under the namespace curl in cluster cluster-2, which is managed by the mesh, and the injected sidecar will be fully traffic dispatched across the cluster. Specify here to use ServiceAccout curl.\nexport NAMESPACE=curl kubectx k3d-cluster-2 kubectl create namespace ${NAMESPACE} osm namespace add ${NAMESPACE} kubectl apply -n ${NAMESPACE} -f - \u003c\u003cEOF apiVersion: v1 kind: ServiceAccount metadata: name: curl --- apiVersion: v1 kind: Service metadata: name: curl labels: app: curl service: curl spec: ports: - name: http port: 80 selector: app: curl --- apiVersion: apps/v1 kind: Deployment metadata: name: curl spec: replicas: 1 selector: matchLabels: app: curl template: metadata: labels: app: curl spec: serviceAccountName: curl containers: - image: curlimages/curl imagePullPolicy: IfNotPresent name: curl command: [\"sleep\", \"365d\"] EOF sleep 3 kubectl wait --for=condition=ready pod -n ${NAMESPACE} --all --timeout=60s Export service export NAMESPACE_MESH=httpbin for CLUSTER_NAME in cluster-1 cluster-3 do kubectx k3d-${CLUSTER_NAME} kubectl apply -f - \u003c\u003cEOF apiVersion: flomesh.io/v1alpha1 kind: ServiceExport metadata: namespace: ${NAMESPACE_MESH} name: httpbin spec: serviceAccountName: \"httpbin\" rules: - portNumber: 8080 path: \"/${CLUSTER_NAME}/httpbin-mesh\" pathType: Prefix --- apiVersion: flomesh.io/v1alpha1 kind: ServiceExport metadata: namespace: ${NAMESPACE_MESH} name: httpbin-${CLUSTER_NAME} spec: serviceAccountName: \"httpbin\" rules: - portNumber: 8080 path: \"/${CLUSTER_NAME}/httpbin-mesh-${CLUSTER_NAME}\" pathType: Prefix EOF sleep 1 done Test We switch back to the cluster cluster-2.\nkubectx k3d-cluster-2 The default route type is Locality and we need to create an ActiveActive policy to allow requests to be processed using service instances from other clusters.\nkubectl apply -n httpbin -f - \u003c\u003cEOF apiVersion: flomesh.io/v1alpha1 kind: GlobalTrafficPolicy metadata: name: httpbin spec: lbType: ActiveActive targets: - clusterKey: default/default/default/cluster-1 - clusterKey: default/default/default/cluster-3 EOF In the curl application of the cluster-2 cluster, we send a request to httpbin.httpbin.\ncurl_client=\"$(kubectl get pod -n curl -l app=curl -o jsonpath='{.items[0].metadata.name}')\" kubectl exec \"${curl_client}\" -n curl -c curl -- curl -s http://httpbin.httpbin:8080/ A few more requests will see the following response.\nHi, I am from cluster-1 and controlled by mesh! Hi, I am from cluster-2 and controlled by mesh! Hi, I am from cluster-3 and controlled by mesh! Hi, I am from cluster-1 and controlled by mesh! Hi, I am from cluster-2 and controlled by mesh! Hi, I am from cluster-3 and controlled by mesh! Demo Adjusting the traffic policy mode Let’s adjust the traffic policy mode of cluster cluster-2 so that the traffic policy can be applied.\nkubectx k3d-cluster-2 export osm_namespace=osm-system kubectl patch meshconfig osm-mesh-config -n \"$osm_namespace\" -p '{\"spec\":{\"traffic\":{\"enablePermissiveTrafficPolicyMode\":false}}}' --type=merge In this case, if you try to send the request again, you will find that the request fails. This is because in traffic policy mode, inter-application access is prohibited if no policy is configured.\nkubectl exec \"${curl_client}\" -n curl -c curl -- curl -s http://httpbin.httpbin:8080/ command terminated with exit code 52 Application Access Control Policy The access control policy of SMI is based on ServiceAccount as mentioned at the beginning of the guide, that’s why our deployed httpbin service uses different ServiceAccount in cluster cluster-1, cluster-3, and cluster cluster-2.\n cluster-1：httpbin cluster-2：default cluster-3：httpbin  Next, we will set different access control policies TrafficTarget for in-cluster and out-of-cluster services, differentiated by the ServiceAccount of the target load in TrafficTarget.\nExecute the following command to create a traffic policy curl-to-httpbin that allows curl to access loads under the namespace httpbin that uses ServiceAccount default.\nkubectl apply -n httpbin -f - \u003c\u003cEOF apiVersion: specs.smi-spec.io/v1alpha4 kind: HTTPRouteGroup metadata: name: httpbin-route spec: matches: - name: all pathRegex: \"/\" methods: - GET --- kind: TrafficTarget apiVersion: access.smi-spec.io/v1alpha3 metadata: name: curl-to-httpbin spec: destination: kind: ServiceAccount name: default namespace: httpbin rules: - kind: HTTPRouteGroup name: httpbin-route matches: - all sources: - kind: ServiceAccount name: curl namespace: curl EOF Multiple request attempts are sent and the service of cluster cluster-2 responds, while clusters cluster-1 and cluster-3 will not participate in the service.\nHi, I am from cluster-2 and controlled by mesh! Hi, I am from cluster-2 and controlled by mesh! Hi, I am from cluster-2 and controlled by mesh! Execute the following command to check ServiceImports and you can see that cluster-1 and cluster-3 export services using ServiceAccount httpbin.\nkubectl get serviceimports httpbin -n httpbin -o jsonpath='{.spec}' | jq { \"ports\": [ { \"endpoints\": [ { \"clusterKey\": \"default/default/default/cluster-1\", \"target\": { \"host\": \"192.168.1.110\", \"ip\": \"192.168.1.110\", \"path\": \"/cluster-1/httpbin-mesh\", \"port\": 81 } }, { \"clusterKey\": \"default/default/default/cluster-3\", \"target\": { \"host\": \"192.168.1.110\", \"ip\": \"192.168.1.110\", \"path\": \"/cluster-3/httpbin-mesh\", \"port\": 83 } } ], \"port\": 8080, \"protocol\": \"TCP\" } ], \"serviceAccountName\": \"httpbin\", \"type\": \"ClusterSetIP\" } So, we create another TrafficTarget curl-to-ext-httpbin that allows curl to access the load using ServiceAccount httpbin.\nkubectl apply -n httpbin -f - \u003c\u003cEOF kind: TrafficTarget apiVersion: access.smi-spec.io/v1alpha3 metadata: name: curl-to-ext-httpbin spec: destination: kind: ServiceAccount name: httpbin namespace: httpbin rules: - kind: HTTPRouteGroup name: httpbin-route matches: - all sources: - kind: ServiceAccount name: curl namespace: curl EOF After applying the policy, test it again and all requests are successful.\nHi, I am from cluster-2 and controlled by mesh! Hi, I am from cluster-1 and controlled by mesh! Hi, I am from cluster-3 and controlled by mesh! ","excerpt":"Pre-requisites  Tools and clusters created in demo Multi-cluster services discovery \u0026 communication  This guide will expand on the knowledge we covered in previous guides and demonstrate how to …","ref":"/docs/demos/multicluster_services_access_control/","title":"Multi-cluster services access control"},{"body":"","excerpt":"","ref":"/categories/","title":"Categories"},{"body":"","excerpt":"","ref":"/docs/","title":"Docs"},{"body":"FSM(Flomesh Service Mesh ) is Kubernetes North-South traffic manager, provides Ingress controllers, Gateway API, Load Balancer, and cross-cluster service registration and service discovery. FSM uses Pipy as data plane and suitable for cloud, edge and IoT.\n","excerpt":"FSM(Flomesh Service Mesh ) is Kubernetes North-South traffic manager, provides Ingress controllers, Gateway API, Load Balancer, and cross-cluster service registration and service discovery. FSM uses …","ref":"/","title":"FSM Documentation"},{"body":"","excerpt":"","ref":"/tags/","title":"Tags"}]